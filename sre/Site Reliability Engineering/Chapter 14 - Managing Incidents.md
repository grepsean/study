# Managing Incidents

효율적인 장애 관리의 핵심은 빠르게 normal 상태로 복구하는 것이다. 만약 potential 장애 처리에 대해 미리 대응 메뉴얼을 만들어 두지 않았다면, 원칙적인 장애 관리는 쓸모없어질 것이다.

이 챕터에서는 ad hoc한 장애 관례를 인해서 통제할 수 없는 장애가 발생한 것에 대해서 살펴볼 것이다. 이러한 장애를 잘 관리할 수 있는 방법을 살펴볼 것이다.

# Unmanaged Incidents
Mary라는 on-call 엔지니어 입장이 되어보자. 묙요일 오후 2시가 되면 노티(pager) 폭탄을 맞게 된다. Black-box 모니터링을 통해서 전체 데이터 센터에서 어느 트래픽에 대해서 장애가 발생을 인지했다. 한숨을 쉬는 것을 시작으로 커피를 손에서 내려놓고 복구를 시작한다. 작업을 몇분 진행하는 동안 또 다른 장애 알람이 들어온다. 그리고 5개의 데이터센터중 3개에서 다운된다. 상황은 더 악화되어 살아 있는 데이터센터는 자신의 capa보다 많은 트래픽을 처리하면서 과부하가 발생하고 결국 정상적인 서비스가 불가능해진다. 

로그를 처다보고 있지만 영원히 끝나지 않을 정도로 쏟아진다. 수천 라인의 로그 속에서 최근에 업데이트한 모듈에 문제가 있다는 힌트를 얻었고 이전 릴리즈로 롤백을 하기로 결정한다. 그리곤 롤백도 도움이 되지 않는다는것을 알게되고 대부분의 코드를 작성한 Josephine을 호출한다. 그녀의 timezone에서는 새벽 3:30이라는것을 상기시키며, 그녀는 눈을 비비며 로그를 같이 살펴보기로 한다. 동료인 Sabrina, Robin도 같이 조금 살펴보겠다고 한다.(Just looking)

그리고 boss가 화가나서 왜 "total meltdown of this business-critical service."에 대해서 보고 받지 못했는지 이유를 요구한다. 동시에 부사장에게도 "How could this possibl have happened"를 반복적으로 불평하며 ETA를 요구한다. VPs call 중 이전 엔지니어링 경험으로 "Increase the page size!"라는 관련은 없어보이지만 거부할 수 없는 요청을 받는다. 

시간이 지나 나머지 두 데이터 센터도 다운되었다. 잠에서 덜깬 Josephoine은 Mary 모르게 Macolm을 호출한다. 
Macolm은 CPU affinity에 대해서 의심을 했으며, 자신이 서버 프로세스에 대해서 최적화할 수 있고 심플한 변경을 프로덕션에 배포만 하면 된다고 확신했다. 그리고 그렇게 했다. 
몇초 후 서버가 재시작되어 변경사항이 반영되었지만 결국 죽었다.


# The Anatomy of an Unmanaged Incident
위의 시나리오 처럼 모두가 자기 일을 했지만 상황은 왜 나빠졌을까? 몇가지 위험 요소가 결국 통제 불능을 야기 시켰다.

## Sharp Focus on the Technical Problem
보통 Mary처럼 technical 역량를 가진 사람들을 고용한다. 그녀가 문제를 해결하기 위해서 운영적인 변경을 하는데 바빴다는 것은 전혀 놀랍지 않은 일이다. 
그리고 그녀는 technical task가 너무 많기 때문에 이러한 문제를 어떻게 해결할지 빅픽처를 생각할 위치의 사람은 아니다. 

## Poor Communication
비슷한 이유로 Mary는 명확하게 커뮤니케이션할 수 없을 정도로 바빴다. 동료들이 어떤 액션을 취했는지 누구도 알지 못했다. 비즈니스 리더는 화가 났고, 고객은 짜증이 났다. 그리고 같이 디버깅하고 복구하는데 다른 엔지니어들의 도움이 효율적으로 활용되지 않았다.

## Freelancing
Malcolm은 자신의 최선의 의도로 시스템 변경을 했지만, 트러블슈팅의 메인 담당자인 메리와 조차 협업을 이끌지 못했다. 그의 변경은 결국 상황을 더 악화시켰다.



# Elements of Incident Management Process
장애 관리의 skill과 practice는 각 개인의 열정적인 노력을 이어지게 하도록 한다. 구글의 장애 관리 시스템은 [Incident Command System] 이라는 것을 기반으로 한다. 
잘 디자인된 장애 관리 프로세스는 아래와 같은 피쳐를 가진다.

## Recursive Separation of Responsibilities
장에와 관련된 모든 사람들은 자신의 role을 명확히 알고, 다른 사람의 role을 침범하지 않아야한다. 
책임을 명확히 분리하면 그들의 동료의 second-guess가 필요하지 않으므로 각 개인은 더 자율성을 가지게 된다. 

어떤 사람의 로드가 과부화되면, 그 사람은 더 많은 사람이 필요하다고 리더에게 요청해야하고, 그 일을 다른 사람에게 위임해야 하는데 이 일로 인해 또 다른 sub 장애가 발생할 수 있다. 또는 


### Incident Command
장애 관리자(commander)은 장애에 대한 high level 상태를 관리한다. 장애 대응을 위한 task force를 구성하고, 필요와 우선 순위에 따라 책임을 할당한다. 사실상 표준으로 관리자는 delagated되지 않은 모든 포지션을 담당한다. 가능하다면 Ops팀이 효율적으로 동작하게 하지 막는 장벽을 없앨 수 있다.

### Operational Work
Ops의 리더는 장애 관리자와 협업하여 operational tools를 적용하여 장애 대응을 할 수 있다. 운영팀은 장애동안 시스템을 수정할 수 있는 유일한 그룹이어야 한다. 

### Communication
이 사람은 장애 대응 task force의 첫번째 컨텍 포인트이다. 그들의 권한은 장애 대응팀과 이해 관계자에게 이슈에 대한 주기적은 업데이트를 해주는 것이다. (주로 이메일을 통해) 그리고 장애 보고서를 정확하고 최신으로 업데이트하는 일로도 확장될 수 있다.

### Planning
Planning role은 longer-term 이슈를 처리하므로써 Ops팀을 서포트한다. 예를 들면, 버그를 올리고, 저녁 식사 주문하고, handoff를 arraging하고, 정상 상태와 비교에 어떻게 다른지 tracking해서 장애 복구되면 revert될 수 있게 한다.


## A Recognized Command Post
관련된 그룹들은 장애 관리자와 소통할 수 있는 곳을 알고 있어야한다. 많은 상황에서, 장애 TF 멤버들은 "War Room"이라는 곳에 위치해 있다. 다른 팀들은 각자의 선호하는 곳에 위치해있을 것이고 장애 업데이트 메일/공지을 주시하고 있을 것이다.

구글은 장애 대응에 IRC를 사용한다. IRC는 매우 reliable하고, 어떤 이벤트의 커뮤니케이션 로그로 활용할 수 있다. 또한 장애 관련 트래픽을 로깅(postmorterm에 유용함)할 수 있는 봇을 만들고 로그 이벤트를 채널로 alert할 수 있다.
IRC는 또한 지역적으로 분산되어 있는 팀이 협업하기에 유용하다.

## Live Incident State Document
장애 관리자의 가장 중요한 책임은 장애 보고서를 최신으로 유지하는 것이다. 이 문서는 wiki에 있을 수도 있지만, 여러명이 동시에 편진할 수 있어야 한다. 팀의 대부분은 Google Docs을 이용하고, Google Docs SRE는 Google Sites를 이용한다. 

장애 문서 예제 [Example Incident State Document](https://sre.google/sre-book/incident-document/)를 살펴보면, 좀 지저분할 수 있지만 기능적이어야 한다. 
Template을 이용하면 좀더 이러한 문서를 쉽게 만들어 낼 수 있고, 가장 중요한 정보를 최상단에 위치시켜 더 유용하게 해준다. 
이러한 문서는 postmortem analysis이나 메타 분석위해서 유지하자.


## Clear, Live Handoff
working day가 끝나면 다음 장애 관리자에게 명확하게 인계하는것은 필수적이다. 만약 다른 위치에 있는 관리자에게 인계할때는, phone이나 video call을 통해서 새로운 장애 관리자를 간단하고 안전하게 변경할 수 있다. 새로운 장애 관리자가 통보되면, 이전 관리자는 "당신은 이제 X맨입니다"라고 명확하게 인계를 마무리해야한다. 그리고 인수인계에 대한 응답을 받을때 까지 call에서 나가면 안된다. 인수인계가 마무리되면, 이제 누가 장애 관리에 대해 주도하고 있는지 명확하게 알려야한다.


# A Managed Incident
이제 어떻게 이러한 장애가 장애 관리 원칙에 따라 처리될 수 있는지 살펴보자.

오후 2시 Mary는 3번째 커피 타임을 가지고 있다. pager의 알림은 그녀를 놀라게하고, 커피를 홀랑 마셔버린다.
그리곤 한 데이터 센터가 다운된다. 그녀는 investigate를 시작한다. 잠시후 다른 alert을 수신하는데 두번째 데이터센터도 다운된다. 
빠르게 이슈가 늘어 났기 때문에, 그녀는 장애 관리 프레임워크의 구조가 그녀에게 도움이 된다는것을 알고 있다.

Mary는 Sabrina에게 연락해서 "Comand 받을 수 있나요?" 동의하면 Sabrina는 Mary에게 현재까지 일어난 일에 대해 빠르게 전달받는다. 그녀는 이러한 디테일을 이메일을 통해서 미리 정해진 mailing list로 전송한다. Sabrina는 그녀가 아직 장애의 영향을 파악하지 못했다는것 인지하고, Mary에게 의견을 요청한다. Mary는 "사용자들은 아직 영향받지 않고, 3번째 데이터 센터를 잃으면 안된다"라고 응답한다. Sabrina는 Mary의 응답을 장애 문서에 바로 기록한다.

3번째 alert이 오면, Sabrina는 IRC를 통해서 alert을 확인하고, 빠르게 이메일 쓰레드를 확인한다. Sabrina는 외부 communication 대표에게 user messaging 메시지 준비를 시작하도록 요청한다. 그녀는 Mary에게 연락하여 developer on-call (currently Josephine)에게 연락해야할지 확인한다. Mary의 승인을 받아 Josephine에게 연락한다. 

Josephine은 로그인하고, Robin도 자원한다. Sabrina는 Robin and Josephine에게 Mary가 그들에게 위임 한 작업의 우선 순위를 지정하는것을 리마인드하고, 추가로 취하는 액션들을 Mary에게 지속적으로 알려야한다는 것을 알린다.
Robin and Josephine은 장애 문서를 읽고 현재 상황에 대해서 숙지한다. 

지금까지, Mary는 예전 바이너리 릴리즈를 시도해보고 이것으로 해결되지 않는다는 것을 Robin에게 알렸고, Robin은 이 내용을 IRC를 통해 알렸다. Sabrina는 이러한 내용을 장애 문서에 업데이트한다. 

5PM에, Sabrina는 장애를 대응할 대체 지원을 찾기 시작한다, 그녀와 그가 집에 갈 시간이 되었기 때문이다. 그녀는 장애 문서에 업데이트를 한다. 5:45에 phone conference를 통해서 모두에게 현재 상황을 알린다. 6PM에, 그들은 다른 사무실에 있는 동료에게 인수인계한다.

Mary는 다음날 아침에 복귀하여 대서양 넘어의 동료가 이 버그에 대한 책임이 맡아서, 문제를 완화시키고, 장애를 닫고, postmorterm을 시작했다는것을 확인했다. 문제는 해결되고 그녀는 신선한 커피를 끓이고 구조적 개선을 계획하는 것으로 이 문제로 팀이 다시 괴롭힘 받지 않도록 했다.


# When to Declare an Incident
문제가 급증하도록 장애 관리 프레임워크 시간이 증가하게 두는 것보다는 장애를 먼저 선언하고, 간단한 fix를 찾아 장애를 종료시키는게 더 좋다. 선언된 장애에 대한 명확한 상태를 설정한다. 팀은 가이드라인에 따라 아래 중 하나가 참이라면, 그 이벤트는 장애일 것이다.
* 문제를 해결하기 위해서 두번째 팀이 참여해야하나?
* 장애(outage)가 고객에게 보여지는가?
* 한시간의 집중적 분석으로도 해결되지 않는 이슈인가?

장애 관리 능력은 사용되지 않으면 위축된다. 따라서 어떻게 엔지니어들이 이러한 장애관리 스킬을 잘 유지할 수 있을까? 
운이 좋게도, 장애 관리 프레임워크는 타임존이나 팀에 걸쳐 운영상 변경이 일어날때에도 적용해볼 수 있다. 이 프레임워크를 일반적인 변경 관리 절차처럼 잘 사용하고 있다면, 실제 장애가 일어날때에도 쉽게 이 프레임워크를 사용할 수 있다. 
만약 당신의 조직이 disaster-recovery testing을 한다면, 장애 관리는 테스팅 과정의 일부여야한다. 우리는 더 익숙해지기 위해서 이미 해결된 on-call 이슈에 대해서 자주 role-play를 해볼 수 있다. 


# In Summary
사전에 장애 관리 전략을 세우고, 계획을 원활하게 확장하도록 구조화하고, 계획을 정기적으로 사용하므로써 복구 시간을 단축할 수 있고, 스탭들에게는 emergent 문제를 해결하면서 스트레스를 적게받게 할 수 있다. reliability를 고려하고 있는 어떤 조직이든 비슷한 전략을 추구하는 것이 이점이 있다.

## Best Practices for Incident Management
* Prioritize : 그만 피를 흘리고, 서비스를 복구하고, 원인을 찾아 보존해야한다.
* Prepare : 장애 관련자들과 협의하여 사전에 장애 관리 절차에 대한 개발과 문서화한다.
* Trust : 장애 관련자들에게 각자의 롤 안에서 full 자율성을 부여한다.
* Introspect : 장애 대응하는 동안은 감정 상태에 주의를 해야한다. 패닉에 빠지거나 장애에 압도당하는 경우에는 도움을 요청한다. 
* Consider alternatives : 현재 진행중인 해결책이 여전히 타당한지 다를 방법을 취해야하는지 주기적으로 옵션을 재고/재평가해본다. 
* Practice : 주기적으로 절차를 사용보면, second nature가 된다.
* Change it around : 가장 마지막에 장애 관리자를 였다면, 이번에는 다른 역할을 맡아야한다. 모든 팀 멤버가 모든 역할에 친숙해지는 것을 권장한다.

